{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2yTXAw7Xx-W",
        "outputId": "8d9ed7a1-a5f6-4663-e0e2-8378477c50bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.0\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp312-cp312-linux_x86_64.whl size=2399004 sha256=3e507bb57d81f0a9007f98609836909488db87209e8ce28728f0216e8991f4c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/3c/ab/b6fd75459cadc56f4a4125d4cb387a708a59ca8589e4cc6b7d\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym[box2d]) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym[box2d]) (0.1.0)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0.tar.gz (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "! pip install swig\n",
        "! pip install box2d-py\n",
        "! pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/BipedalWalker_Project\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "print(f\"✅ Saving all models to: {DRIVE_FOLDER}\")\n",
        "\n",
        "def show_video(video_folder=\"videos\"):\n",
        "    mp4list = glob.glob(f'{video_folder}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = max(mp4list, key=os.path.getctime)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"No video found yet.\")\n",
        "\n",
        "# Model architecture\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 1), std=1.0),\n",
        "        )\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, act_dim), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.ones(1, act_dim) * -0.5)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "        dist = Normal(action_mean, action_std)\n",
        "\n",
        "        if action is None:\n",
        "            action = dist.sample()\n",
        "\n",
        "        log_prob = dist.log_prob(action).sum(1)\n",
        "        entropy = dist.entropy().sum(1)\n",
        "        value = self.critic(x)\n",
        "        return action, log_prob, entropy, value.squeeze(1)\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, size, obs_dim, act_dim, device):\n",
        "        self.size = size\n",
        "        self.device = device\n",
        "        self.obs = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((size, act_dim), dtype=np.float32)\n",
        "        self.log_probs = np.zeros(size, dtype=np.float32)\n",
        "        self.rewards = np.zeros(size, dtype=np.float32)\n",
        "        self.dones = np.zeros(size, dtype=np.float32)\n",
        "        self.values = np.zeros(size, dtype=np.float32)\n",
        "        self.advantages = np.zeros(size, dtype=np.float32)\n",
        "        self.returns = np.zeros(size, dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.path_start_idx = 0\n",
        "\n",
        "    def store(self, obs, action, log_prob, reward, done, value):\n",
        "        assert self.ptr < self.size\n",
        "        self.obs[self.ptr] = obs\n",
        "        self.actions[self.ptr] = action\n",
        "        self.log_probs[self.ptr] = log_prob\n",
        "        self.rewards[self.ptr] = reward\n",
        "        self.dones[self.ptr] = done\n",
        "        self.values[self.ptr] = value\n",
        "        self.ptr += 1\n",
        "\n",
        "    def finish_path(self, last_value, gamma, lam):\n",
        "        path_slice = slice(self.path_start_idx, self.ptr)\n",
        "        rewards = np.append(self.rewards[path_slice], last_value)\n",
        "        values = np.append(self.values[path_slice], last_value)\n",
        "\n",
        "        gae = 0.0\n",
        "        adv = np.zeros_like(self.rewards[path_slice])\n",
        "\n",
        "        for t in reversed(range(len(rewards) - 1)):\n",
        "            delta = rewards[t] + gamma * values[t + 1] * (1 - self.dones[path_slice][t]) - values[t]\n",
        "            gae = delta + gamma * lam * (1 - self.dones[path_slice][t]) * gae\n",
        "            adv[t] = gae\n",
        "\n",
        "        self.advantages[path_slice] = adv\n",
        "        self.returns[path_slice] = adv + self.values[path_slice]\n",
        "        self.path_start_idx = self.ptr\n",
        "\n",
        "    def get(self):\n",
        "        assert self.ptr == self.size\n",
        "        self.ptr = 0\n",
        "        self.path_start_idx = 0\n",
        "\n",
        "        adv = self.advantages\n",
        "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "\n",
        "        return dict(\n",
        "            obs=torch.tensor(self.obs, dtype=torch.float32, device=self.device),\n",
        "            actions=torch.tensor(self.actions, dtype=torch.float32, device=self.device),\n",
        "            log_probs=torch.tensor(self.log_probs, dtype=torch.float32, device=self.device),\n",
        "            advantages=torch.tensor(adv, dtype=torch.float32, device=self.device),\n",
        "            returns=torch.tensor(self.returns, dtype=torch.float32, device=self.device),\n",
        "            values=torch.tensor(self.values, dtype=torch.float32, device=self.device),\n",
        "        )\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_id=\"BipedalWalker-v3\",\n",
        "        total_timesteps=2_000_000,\n",
        "        rollout_steps=4096,\n",
        "        gamma=0.99,\n",
        "        lam=0.95,\n",
        "        clip_eps=0.2,\n",
        "        learning_rate=2.5e-4,\n",
        "        train_epochs=10,\n",
        "        minibatch_size=512,\n",
        "        vf_coef=0.5,\n",
        "        ent_coef=0.00,\n",
        "        render_freq=25,\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        self.env_id = env_id\n",
        "        self.total_timesteps = total_timesteps\n",
        "        self.rollout_steps = rollout_steps\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.clip_eps = clip_eps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.train_epochs = train_epochs\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.vf_coef = vf_coef\n",
        "        self.ent_coef = ent_coef\n",
        "        self.render_freq = render_freq\n",
        "        self.device = device\n",
        "\n",
        "        # Stat tracking for plots\n",
        "        self.ep_returns = []\n",
        "        self.ep_timesteps = []\n",
        "\n",
        "        self.env = gym.make(env_id)\n",
        "        self.env = gym.wrappers.RecordEpisodeStatistics(self.env)\n",
        "        self.env = gym.wrappers.ClipAction(self.env)\n",
        "\n",
        "        self.env = gym.wrappers.NormalizeObservation(self.env)\n",
        "        self.env = gym.wrappers.TransformObservation(self.env, lambda obs: np.clip(obs, -10, 10), self.env.observation_space)\n",
        "\n",
        "        self.obs_dim = self.env.observation_space.shape[0]\n",
        "        self.act_dim = self.env.action_space.shape[0]\n",
        "\n",
        "        self.ac = ActorCritic(self.obs_dim, self.act_dim).to(device)\n",
        "        self.optimizer = optim.Adam(self.ac.parameters(), lr=self.learning_rate, eps=1e-5)\n",
        "\n",
        "        self.num_updates = total_timesteps // rollout_steps\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            self.optimizer, start_factor=1.0, end_factor=0.0, total_iters=self.num_updates\n",
        "        )\n",
        "\n",
        "        self.buffer = RolloutBuffer(self.rollout_steps, self.obs_dim, self.act_dim, self.device)\n",
        "\n",
        "    def visualize_agent(self, update_count):\n",
        "        print(f\"\\n--- Visualizing Agent at Update {update_count} ---\")\n",
        "        vis_env = gym.make(self.env_id, render_mode=\"rgb_array\")\n",
        "        vis_env = gym.wrappers.RecordVideo(\n",
        "            vis_env,\n",
        "            video_folder=\"videos\",\n",
        "            name_prefix=f\"update_{update_count}\",\n",
        "            disable_logger=True\n",
        "        )\n",
        "        vis_env = gym.wrappers.ClipAction(vis_env)\n",
        "        vis_norm = gym.wrappers.NormalizeObservation(vis_env)\n",
        "        try:\n",
        "            vis_norm.obs_rms = self.env.get_wrapper_attr('obs_rms')\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        vis_env = gym.wrappers.TransformObservation(vis_norm, lambda obs: np.clip(obs, -10, 10), vis_env.observation_space)\n",
        "\n",
        "        obs, _ = vis_env.reset()\n",
        "        ret = 0\n",
        "        while True:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action = self.ac.actor_mean(obs_tensor).squeeze(0).cpu().numpy()\n",
        "            obs, reward, terminated, truncated, _ = vis_env.step(action)\n",
        "            ret += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        vis_env.close()\n",
        "        print(f\"Visualization finished with return: {ret:.2f}\")\n",
        "        show_video(\"videos\")\n",
        "\n",
        "    def train(self):\n",
        "        obs, _ = self.env.reset()\n",
        "        timesteps_collected = 0\n",
        "        update_count = 0\n",
        "\n",
        "        while timesteps_collected < self.total_timesteps:\n",
        "            for _ in range(self.rollout_steps):\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    action, log_prob, _, value = self.ac.get_action_and_value(obs_tensor)\n",
        "\n",
        "                action = action.cpu().numpy().squeeze(0)\n",
        "                log_prob = log_prob.item()\n",
        "                value = value.item()\n",
        "\n",
        "                next_obs, reward, terminated, truncated, infos = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                self.buffer.store(obs, action, log_prob, reward, done, value)\n",
        "                timesteps_collected += 1\n",
        "                obs = next_obs\n",
        "\n",
        "                if \"episode\" in infos:\n",
        "                    ret = infos['episode']['r']\n",
        "                    self.ep_returns.append(ret)\n",
        "                    self.ep_timesteps.append(timesteps_collected)\n",
        "                    print(f\"Update {update_count} | Steps: {timesteps_collected} | Return: {ret:.2f}\")\n",
        "\n",
        "                if done:\n",
        "                    if truncated:\n",
        "                        last_val_obs = torch.tensor(next_obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                        with torch.no_grad():\n",
        "                            last_value = self.ac.get_value(last_val_obs).item()\n",
        "                    else:\n",
        "                        last_value = 0\n",
        "                    self.buffer.finish_path(last_value=last_value, gamma=self.gamma, lam=self.lam)\n",
        "                    obs, _ = self.env.reset()\n",
        "\n",
        "                if timesteps_collected >= self.total_timesteps:\n",
        "                    break\n",
        "\n",
        "            if not done:\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    last_value = self.ac.get_value(obs_tensor).item()\n",
        "                self.buffer.finish_path(last_value=last_value, gamma=self.gamma, lam=self.lam)\n",
        "\n",
        "            if self.buffer.ptr == self.buffer.size:\n",
        "                data = self.buffer.get()\n",
        "                self._update(data)\n",
        "                self.lr_scheduler.step()\n",
        "                update_count += 1\n",
        "\n",
        "                if update_count % 50 == 0:\n",
        "                    self.save(os.path.join(DRIVE_FOLDER, \"ppo_bipedal_checkpoint.pt\"))\n",
        "                    print(f\"Checkpoint saved to drive at update {update_count}\")\n",
        "\n",
        "            if update_count > 0 and update_count % self.render_freq == 0:\n",
        "                self.visualize_agent(update_count)\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def _update(self, data):\n",
        "        obs = data[\"obs\"]\n",
        "        actions = data[\"actions\"]\n",
        "        old_log_probs = data[\"log_probs\"]\n",
        "        advantages = data[\"advantages\"]\n",
        "        returns = data[\"returns\"]\n",
        "\n",
        "        batch_size = len(obs)\n",
        "        inds = np.arange(batch_size)\n",
        "\n",
        "        for _ in range(self.train_epochs):\n",
        "            np.random.shuffle(inds)\n",
        "            for start in range(0, batch_size, self.minibatch_size):\n",
        "                end = start + self.minibatch_size\n",
        "                mb_inds = inds[start:end]\n",
        "                mb_obs = obs[mb_inds]\n",
        "                mb_actions = actions[mb_inds]\n",
        "                mb_old_log_probs = old_log_probs[mb_inds]\n",
        "                mb_adv = advantages[mb_inds]\n",
        "                mb_returns = returns[mb_inds]\n",
        "\n",
        "                _, new_log_probs, entropy, values = self.ac.get_action_and_value(mb_obs, mb_actions)\n",
        "                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
        "                surr1 = ratio * mb_adv\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * mb_adv\n",
        "\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_loss = 0.5 * ((values - mb_returns) ** 2).mean()\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = actor_loss + self.vf_coef * critic_loss - self.ent_coef * entropy_loss\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.ac.parameters(), max_norm=0.5)\n",
        "                self.optimizer.step()\n",
        "\n",
        "    def plot_results(self, window_size=50):\n",
        "        if not self.ep_returns:\n",
        "            print(\"Can not plot, no rewards\")\n",
        "            return\n",
        "\n",
        "        # Calculate Running Average\n",
        "        returns = np.array(self.ep_returns)\n",
        "        running_avg = np.convolve(returns, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.ep_timesteps, returns, alpha=0.3, color='blue', label='Episode Return')\n",
        "\n",
        "        avg_timesteps = self.ep_timesteps[window_size-1:]\n",
        "        plt.plot(avg_timesteps, running_avg, color='red', linewidth=2, label=f'Running Avg (last {window_size} eps)')\n",
        "\n",
        "        plt.title(f\"PPO Training Progress: {self.env_id}\")\n",
        "        plt.xlabel(\"Total Timesteps\")\n",
        "        plt.ylabel(\"Return\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        # Save plot to drive\n",
        "        plot_path = os.path.join(DRIVE_FOLDER, \"training_plot.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.show()\n",
        "        print(f\"Plot saved to: {plot_path}\")\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.ac.state_dict(), path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    agent = PPOAgent(\n",
        "        env_id=\"BipedalWalker-v3\",\n",
        "        total_timesteps=2_000_000,\n",
        "        rollout_steps=4096,\n",
        "        render_freq=25,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    agent.train()\n",
        "\n",
        "    # Final plot and save\n",
        "    agent.plot_results(window_size=50)\n",
        "\n",
        "    final_model_path = os.path.join(DRIVE_FOLDER, \"ppo_bipedal_final.pt\")\n",
        "    final_stats_path = os.path.join(DRIVE_FOLDER, \"obs_stats.pkl\")\n",
        "    agent.save(final_model_path)\n",
        "\n",
        "    with open(final_stats_path, \"wb\") as f:\n",
        "        pickle.dump(agent.env.get_wrapper_attr('obs_rms'), f)\n",
        "\n",
        "    print(f\"Training Complete. Files saved to Drive:\\n1. {final_model_path}\\n2. {final_stats_path}\")"
      ],
      "metadata": {
        "id": "s8Hdj53q6jmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for consistent finisher part 1, firstly the agent is trained using reward and observation normalisation ###"
      ],
      "metadata": {
        "id": "iha_a5Fr7Pul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/BipedalWalker_Project\"\n",
        "BASE_MODEL = os.path.join(DRIVE_FOLDER, \"ppo_bipedal_final.pt\")\n",
        "BASE_STATS = os.path.join(DRIVE_FOLDER, \"obs_stats.pkl\")\n",
        "CHECKPOINT = os.path.join(DRIVE_FOLDER, \"hardcore_checkpoint.pt\")\n",
        "FINAL_SAVE = os.path.join(DRIVE_FOLDER, \"ppo_hardcore_solved.pt\")\n",
        "\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "def show_video(folder=\"videos\"):\n",
        "    mp4list = glob.glob(f'{folder}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = max(mp4list, key=os.path.getctime)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''.format(encoded.decode('ascii'))))\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 1), std=1.0),\n",
        "        )\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, act_dim), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.ones(1, act_dim) * -0.5)\n",
        "\n",
        "    def get_value(self, x): return self.critic(x)\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "        dist = Normal(action_mean, action_std)\n",
        "        if action is None: action = dist.sample()\n",
        "        return action, dist.log_prob(action).sum(1), dist.entropy().sum(1), self.critic(x).squeeze(1)\n",
        "\n",
        "class HardcoreAgent:\n",
        "    def __init__(self, device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.total_timesteps = 40_000_000\n",
        "        self.rollout_steps = 4096\n",
        "        self.learning_rate = 1.0e-4\n",
        "        self.ent_coef_start = 0.01\n",
        "        self.ent_coef_end = 0.00\n",
        "        self.gamma = 0.999\n",
        "        self.lam = 0.95\n",
        "        self.clip_eps = 0.1\n",
        "        self.train_epochs = 10\n",
        "        self.minibatch_size = 512\n",
        "        self.vf_coef = 0.5\n",
        "        self.max_grad_norm = 0.5\n",
        "\n",
        "        self.history = {'steps': [], 'returns': [], 'phase': []}\n",
        "\n",
        "        self.start_timestep = 0\n",
        "        self.start_update = 0\n",
        "        self.success_count = 0\n",
        "\n",
        "        self.env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
        "        self.env = gym.wrappers.RecordEpisodeStatistics(self.env)\n",
        "        self.env = gym.wrappers.ClipAction(self.env)\n",
        "        self.env = gym.wrappers.NormalizeObservation(self.env)\n",
        "        self.env = gym.wrappers.TransformObservation(self.env, lambda obs: np.clip(obs, -10, 10), self.env.observation_space)\n",
        "        self.env = gym.wrappers.NormalizeReward(self.env, gamma=self.gamma)\n",
        "        self.env = gym.wrappers.TransformReward(self.env, lambda r: np.clip(r, -10, 10))\n",
        "\n",
        "        self.obs_dim = self.env.observation_space.shape[0]\n",
        "        self.act_dim = self.env.action_space.shape[0]\n",
        "\n",
        "        self.ac = ActorCritic(self.obs_dim, self.act_dim).to(device)\n",
        "        self.optimizer = optim.Adam(self.ac.parameters(), lr=self.learning_rate, eps=1e-5)\n",
        "        self.num_updates = self.total_timesteps // self.rollout_steps\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.LinearLR(self.optimizer, start_factor=1.0, end_factor=0.0, total_iters=self.num_updates)\n",
        "\n",
        "        if os.path.exists(CHECKPOINT):\n",
        "            self.load_checkpoint(CHECKPOINT)\n",
        "        elif os.path.exists(BASE_MODEL):\n",
        "            base_weights = torch.load(BASE_MODEL, map_location=device, weights_only=False)\n",
        "            self.ac.load_state_dict(base_weights)\n",
        "            with torch.no_grad(): self.ac.actor_logstd.fill_(-0.5)\n",
        "            if os.path.exists(BASE_STATS):\n",
        "                with open(BASE_STATS, \"rb\") as f: self.set_obs_rms(pickle.load(f))\n",
        "\n",
        "        self.buffer = {\n",
        "            'obs': np.zeros((self.rollout_steps, self.obs_dim), dtype=np.float32),\n",
        "            'actions': np.zeros((self.rollout_steps, self.act_dim), dtype=np.float32),\n",
        "            'log_probs': np.zeros(self.rollout_steps, dtype=np.float32),\n",
        "            'rewards': np.zeros(self.rollout_steps, dtype=np.float32),\n",
        "            'dones': np.zeros(self.rollout_steps, dtype=np.float32),\n",
        "            'values': np.zeros(self.rollout_steps, dtype=np.float32),\n",
        "        }\n",
        "\n",
        "    def get_obs_rms(self):\n",
        "        ptr = self.env\n",
        "        while hasattr(ptr, 'env'):\n",
        "            if isinstance(ptr, gym.wrappers.NormalizeObservation): return ptr.obs_rms\n",
        "            ptr = ptr.env\n",
        "        return None\n",
        "\n",
        "    def set_obs_rms(self, rms):\n",
        "        ptr = self.env\n",
        "        while hasattr(ptr, 'env'):\n",
        "            if isinstance(ptr, gym.wrappers.NormalizeObservation): ptr.obs_rms = rms; break\n",
        "            ptr = ptr.env\n",
        "\n",
        "    def get_return_rms(self):\n",
        "        ptr = self.env\n",
        "        while hasattr(ptr, 'env'):\n",
        "            if isinstance(ptr, gym.wrappers.NormalizeReward): return ptr.return_rms\n",
        "            ptr = ptr.env\n",
        "        return None\n",
        "\n",
        "    def set_return_rms(self, rms):\n",
        "        ptr = self.env\n",
        "        while hasattr(ptr, 'env'):\n",
        "            if isinstance(ptr, gym.wrappers.NormalizeReward): ptr.return_rms = rms; break\n",
        "            ptr = ptr.env\n",
        "\n",
        "    def save_checkpoint(self, path, update, timestep):\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.ac.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.lr_scheduler.state_dict(),\n",
        "            'update_count': update,\n",
        "            'total_timesteps': timestep,\n",
        "            'success_count': self.success_count,\n",
        "            'obs_rms': self.get_obs_rms(),\n",
        "            'return_rms': self.get_return_rms(),\n",
        "            'history': self.history\n",
        "        }\n",
        "        torch.save(checkpoint, path)\n",
        "        self.plot_history()\n",
        "        print(f\"Checkpoint saved and plot updated: Step {timestep}\")\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        ckpt = torch.load(path, map_location=self.device, weights_only=False)\n",
        "        self.ac.load_state_dict(ckpt['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        self.lr_scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "        self.start_update = ckpt['update_count']\n",
        "        self.start_timestep = ckpt['total_timesteps']\n",
        "        self.success_count = ckpt.get('success_count', 0)\n",
        "        self.history = ckpt.get('history', {'steps': [], 'returns': [], 'phase': []})\n",
        "        if ckpt.get('obs_rms'): self.set_obs_rms(ckpt['obs_rms'])\n",
        "        if ckpt.get('return_rms'): self.set_return_rms(ckpt['return_rms'])\n",
        "\n",
        "    def plot_history(self):\n",
        "        if not self.history['returns']: return\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        steps = np.array(self.history['steps'])\n",
        "        returns = np.array(self.history['returns'])\n",
        "        phases = np.array(self.history['phase'])\n",
        "\n",
        "        plt.plot(steps, returns, alpha=0.2, color='blue', label='Raw Episode Return')\n",
        "\n",
        "        if len(returns) > 50:\n",
        "            avg = np.convolve(returns, np.ones(50)/50, mode='valid')\n",
        "            plt.plot(steps[49:], avg, color='red', linewidth=2, label='Running Avg (50)')\n",
        "\n",
        "        if 1 in phases:\n",
        "            idx = np.where(phases == 1)[0][0]\n",
        "            plt.axvline(x=steps[idx], color='green', linestyle='--', linewidth=2)\n",
        "            plt.text(steps[idx], plt.ylim()[1], ' Energy Fine-Tuning Started', color='green', rotation=0, verticalalignment='top')\n",
        "\n",
        "        plt.title(\"BipedalWalker Hardcore: Full Training Progress\")\n",
        "        plt.xlabel(\"Environment Timesteps\")\n",
        "        plt.ylabel(\"Return\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(DRIVE_FOLDER, \"hardcore_training_plot.png\"))\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_agent(self, update_count):\n",
        "        print(f\"\\n--- Visualizing Hardcore Agent ---\")\n",
        "        vis_env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode=\"rgb_array\")\n",
        "        vis_env = gym.wrappers.RecordVideo(vis_env, video_folder=\"videos\", name_prefix=f\"hc_update_{update_count}\", disable_logger=True)\n",
        "        vis_env = gym.wrappers.ClipAction(vis_env)\n",
        "        vis_norm = gym.wrappers.NormalizeObservation(vis_env)\n",
        "        current_obs_rms = self.get_obs_rms()\n",
        "        if current_obs_rms: vis_norm.obs_rms = current_obs_rms\n",
        "        vis_env = gym.wrappers.TransformObservation(vis_norm, lambda obs: np.clip(obs, -10, 10), vis_env.observation_space)\n",
        "        obs, _ = vis_env.reset()\n",
        "        while True:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad(): action = self.ac.actor_mean(obs_tensor).squeeze(0).cpu().numpy()\n",
        "            obs, reward, terminated, truncated, _ = vis_env.step(action)\n",
        "            if terminated or truncated: break\n",
        "        vis_env.close()\n",
        "        show_video(\"videos\")\n",
        "\n",
        "    def train(self):\n",
        "        obs, _ = self.env.reset()\n",
        "        update_count = self.start_update\n",
        "        timesteps_collected = self.start_timestep\n",
        "        while timesteps_collected < self.total_timesteps:\n",
        "            frac = 1.0 - (update_count / self.num_updates)\n",
        "            current_ent_coef = self.ent_coef_start * max(0, frac)\n",
        "            for i in range(self.rollout_steps):\n",
        "                with torch.no_grad():\n",
        "                    obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                    action, log_prob, _, value = self.ac.get_action_and_value(obs_t)\n",
        "                action_np = action.cpu().numpy().squeeze(0)\n",
        "                next_obs, reward, terminated, truncated, infos = self.env.step(action_np)\n",
        "                done = terminated or truncated\n",
        "                self.buffer['obs'][i], self.buffer['actions'][i], self.buffer['log_probs'][i] = obs, action_np, log_prob.item()\n",
        "                self.buffer['rewards'][i], self.buffer['dones'][i], self.buffer['values'][i] = reward, done, value.item()\n",
        "                obs = next_obs\n",
        "                timesteps_collected += 1\n",
        "                if \"episode\" in infos:\n",
        "                    ret = infos['episode']['r']\n",
        "                    self.history['steps'].append(timesteps_collected)\n",
        "                    self.history['returns'].append(ret)\n",
        "                    self.history['phase'].append(0) # Phase 0\n",
        "                    print(f\"Upd {update_count} | Step {timesteps_collected} | Ret: {ret:.2f}\")\n",
        "                    if ret >= 300:\n",
        "                        self.success_count += 1\n",
        "                        if self.success_count >= 20: self.save_checkpoint(FINAL_SAVE, update_count, timesteps_collected); return\n",
        "                if done: obs, _ = self.env.reset()\n",
        "            with torch.no_grad():\n",
        "                next_val = self.ac.get_value(torch.tensor(next_obs, dtype=torch.float32, device=self.device).unsqueeze(0)).item()\n",
        "            adv = np.zeros(self.rollout_steps, dtype=np.float32)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(self.rollout_steps)):\n",
        "                nonterminal = 1.0 - self.buffer['dones'][t]\n",
        "                next_val_t = next_val if t == self.rollout_steps - 1 else self.buffer['values'][t+1]\n",
        "                delta = self.buffer['rewards'][t] + self.gamma * next_val_t * nonterminal - self.buffer['values'][t]\n",
        "                lastgaelam = delta + self.gamma * self.lam * nonterminal * lastgaelam\n",
        "                adv[t] = lastgaelam\n",
        "            returns = adv + self.buffer['values']\n",
        "            b_obs = torch.tensor(self.buffer['obs'], device=self.device)\n",
        "            b_act = torch.tensor(self.buffer['actions'], device=self.device)\n",
        "            b_log = torch.tensor(self.buffer['log_probs'], device=self.device)\n",
        "            b_ret = torch.tensor(returns, device=self.device)\n",
        "            b_adv = torch.tensor(adv, device=self.device)\n",
        "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
        "            inds = np.arange(self.rollout_steps)\n",
        "            for _ in range(self.train_epochs):\n",
        "                np.random.shuffle(inds)\n",
        "                for start in range(0, self.rollout_steps, self.minibatch_size):\n",
        "                    end = start + self.minibatch_size\n",
        "                    mb_inds = inds[start:end]\n",
        "                    _, new_lp, entropy, new_val = self.ac.get_action_and_value(b_obs[mb_inds], b_act[mb_inds])\n",
        "                    ratio = torch.exp(new_lp - b_log[mb_inds])\n",
        "                    surr1, surr2 = ratio * b_adv[mb_inds], torch.clamp(ratio, 1-self.clip_eps, 1+self.clip_eps) * b_adv[mb_inds]\n",
        "                    loss = -torch.min(surr1, surr2).mean() + self.vf_coef * 0.5 * ((new_val.squeeze() - b_ret[mb_inds])**2).mean() - current_ent_coef * entropy.mean()\n",
        "                    self.optimizer.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.ac.parameters(), self.max_grad_norm); self.optimizer.step()\n",
        "            self.lr_scheduler.step(); update_count += 1\n",
        "            if update_count % 100 == 0: self.save_checkpoint(CHECKPOINT, update_count, timesteps_collected); self.visualize_agent(update_count)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = HardcoreAgent(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    agent.train()"
      ],
      "metadata": {
        "id": "3FlCt8zB7JTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for consistent finisher part 2, adds penalties for jerking and over-exertion."
      ],
      "metadata": {
        "id": "iHOghZ7y7_E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import drive\n",
        "\n",
        "# --- DRIVE CONFIG ---\n",
        "if not os.path.exists(\"/content/drive\"): drive.mount(\"/content/drive\")\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/BipedalWalker_Project\"\n",
        "CHECKPOINT_IN = os.path.join(DRIVE_FOLDER, \"hardcore_checkpoint.pt\")\n",
        "CHECKPOINT_OUT = os.path.join(DRIVE_FOLDER, \"hardcore_energy_ft_checkpoint.pt\")\n",
        "FINAL_OUT      = os.path.join(DRIVE_FOLDER, \"ppo_hardcore_energy_ft_final.pt\")\n",
        "BEST_OUT       = os.path.join(DRIVE_FOLDER, \"ppo_hardcore_energy_ft_best.pt\")\n",
        "\n",
        "def show_video(folder=\"videos\"):\n",
        "    mp4list = glob.glob(f\"{folder}/*.mp4\")\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = max(mp4list, key=os.path.getctime)\n",
        "        video = io.open(mp4, \"r+b\").read(); encoded = base64.b64encode(video)\n",
        "        display(HTML(data=f'<video autoplay loop controls style=\"height: 420px;\"><source src=\"data:video/mp4;base64,{encoded.decode(\"ascii\")}\" type=\"video/mp4\" /></video>'))\n",
        "\n",
        "class EnergyPenaltyWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, a_coef=0.015, da_coef=0.035):\n",
        "        super().__init__(env)\n",
        "        self.a_coef, self.da_coef = a_coef, da_coef\n",
        "        self.prev_action = None\n",
        "        self.raw_return = 0.0\n",
        "        self.shaped_return = 0.0\n",
        "        self.ep_len = 0\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.prev_action = np.zeros(self.action_space.shape, dtype=np.float32)\n",
        "        self.raw_return, self.shaped_return, self.ep_len = 0.0, 0.0, 0\n",
        "        return obs, info\n",
        "    def step(self, action):\n",
        "        obs, raw_r, terminated, truncated, info = self.env.step(action)\n",
        "        penalty = self.a_coef * np.mean(np.square(action)) + self.da_coef * np.mean(np.square(action - self.prev_action))\n",
        "        shaped_r = float(raw_r - penalty)\n",
        "        self.prev_action = action.copy()\n",
        "        self.raw_return += float(raw_r); self.shaped_return += float(shaped_r); self.ep_len += 1\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            info = dict(info)\n",
        "            info[\"episode_raw\"] = {\"r\": self.raw_return, \"l\": self.ep_len}\n",
        "            info[\"episode_shaped\"] = {\"r\": self.shaped_return, \"l\": self.ep_len}\n",
        "        return obs, shaped_r, terminated, truncated, info\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std); torch.nn.init.constant_(layer.bias, bias_const); return layer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(layer_init(nn.Linear(obs_dim, 512)), nn.Tanh(), layer_init(nn.Linear(512, 512)), nn.Tanh(), layer_init(nn.Linear(512, 1), std=1.0))\n",
        "        self.actor_mean = nn.Sequential(layer_init(nn.Linear(obs_dim, 512)), nn.Tanh(), layer_init(nn.Linear(512, 512)), nn.Tanh(), layer_init(nn.Linear(512, act_dim), std=0.01))\n",
        "        self.actor_logstd = nn.Parameter(torch.ones(1, act_dim) * -0.5)\n",
        "    def get_value(self, x): return self.critic(x).squeeze(1)\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        mean = self.actor_mean(x); logstd = self.actor_logstd.expand_as(mean); std = torch.exp(logstd); dist = Normal(mean, std)\n",
        "        if action is None: action = dist.sample()\n",
        "        return action, dist.log_prob(action).sum(1), dist.entropy().sum(1), self.critic(x).squeeze(1)\n",
        "\n",
        "class HardcoreEnergyFineTuner:\n",
        "    def __init__(self, device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.total_timesteps, self.rollout_steps = 40_000_000, 4096\n",
        "        self.gamma, self.lam, self.clip_eps, self.learning_rate = 0.99, 0.95, 0.12, 5e-5\n",
        "        self.train_epochs, self.minibatch_size, self.vf_coef, self.max_grad_norm = 10, 512, 0.5, 0.5\n",
        "        self.target_kl, self.a_coef, self.da_coef = 0.02, 0.02, 0.05\n",
        "\n",
        "        self.history = {'steps': [], 'returns': [], 'phase': []} # Will be loaded\n",
        "\n",
        "        env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        env = gym.wrappers.ClipAction(env)\n",
        "        env = gym.wrappers.NormalizeObservation(env)\n",
        "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10), env.observation_space)\n",
        "        self.env = EnergyPenaltyWrapper(env, a_coef=self.a_coef, da_coef=self.da_coef)\n",
        "\n",
        "        self.ac = ActorCritic(self.env.observation_space.shape[0], self.env.action_space.shape[0]).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.ac.parameters(), lr=self.learning_rate, eps=1e-5)\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.LinearLR(self.optimizer, start_factor=1.0, end_factor=0.0, total_iters=self.total_timesteps // self.rollout_steps)\n",
        "\n",
        "        self.buf_obs = np.zeros((self.rollout_steps, self.env.observation_space.shape[0]), dtype=np.float32)\n",
        "        self.buf_act = np.zeros((self.rollout_steps, self.env.action_space.shape[0]), dtype=np.float32)\n",
        "        self.buf_logp, self.buf_rew, self.buf_done, self.buf_val = np.zeros((4, self.rollout_steps), dtype=np.float32)\n",
        "\n",
        "        self.load_checkpoint(CHECKPOINT_IN)\n",
        "\n",
        "    def _get_obs_rms(self):\n",
        "        ptr = self.env\n",
        "        while hasattr(ptr, \"env\"):\n",
        "            if isinstance(ptr, gym.wrappers.NormalizeObservation): return ptr.obs_rms\n",
        "            ptr = ptr.env\n",
        "        return None\n",
        "\n",
        "    def _set_obs_rms(self, rms):\n",
        "        ptr = self.env\n",
        "        while hasattr(ptr, \"env\"):\n",
        "            if isinstance(ptr, gym.wrappers.NormalizeObservation): ptr.obs_rms = rms; return\n",
        "            ptr = ptr.env\n",
        "\n",
        "    def plot_history(self):\n",
        "        if not self.history['returns']: return\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        steps, returns, phases = np.array(self.history['steps']), np.array(self.history['returns']), np.array(self.history['phase'])\n",
        "        plt.plot(steps, returns, alpha=0.2, color='blue', label='Raw Episode Return')\n",
        "        if len(returns) > 50:\n",
        "            avg = np.convolve(returns, np.ones(50)/50, mode='valid')\n",
        "            plt.plot(steps[49:], avg, color='red', linewidth=2, label='Running Avg (50)')\n",
        "\n",
        "        if 1 in phases:\n",
        "            idx = np.where(phases == 1)[0][0]\n",
        "            plt.axvline(x=steps[idx], color='green', linestyle='--', linewidth=2)\n",
        "            plt.text(steps[idx], plt.ylim()[1]*0.9, ' Energy Fine-Tuning Phase', color='green', fontweight='bold')\n",
        "\n",
        "        plt.title(\"Continuous Progress: Hardcore -> Energy Fine-Tuning\")\n",
        "        plt.xlabel(\"Total Environment Timesteps\")\n",
        "        plt.ylabel(\"Raw Return\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(DRIVE_FOLDER, \"hardcore_energy_ft_plot.png\"))\n",
        "        plt.show()\n",
        "\n",
        "    def save_checkpoint(self, path, update, timestep):\n",
        "        ckpt = {'model_state_dict': self.ac.state_dict(), 'optimizer_state_dict': self.optimizer.state_dict(), 'scheduler_state_dict': self.lr_scheduler.state_dict(), 'update_count': update, 'total_timesteps': timestep, 'obs_rms': self._get_obs_rms(), 'history': self.history}\n",
        "        torch.save(ckpt, path)\n",
        "        self.plot_history()\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        ckpt = torch.load(path, map_location=self.device, weights_only=False)\n",
        "        self.ac.load_state_dict(ckpt[\"model_state_dict\"] if \"model_state_dict\" in ckpt else ckpt)\n",
        "        if \"optimizer_state_dict\" in ckpt: self.optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        self.start_update, self.start_timestep = int(ckpt.get(\"update_count\", 0)), int(ckpt.get(\"total_timesteps\", 0))\n",
        "        self.history = ckpt.get('history', {'steps': [], 'returns': [], 'phase': []})\n",
        "        if ckpt.get(\"obs_rms\") is not None: self._set_obs_rms(ckpt[\"obs_rms\"])\n",
        "\n",
        "    def train(self):\n",
        "        obs, _ = self.env.reset(); update_count, timesteps_collected = self.start_update, self.start_timestep\n",
        "        while timesteps_collected < self.total_timesteps:\n",
        "            for t in range(self.rollout_steps):\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                with torch.no_grad(): action, logp, _, value = self.ac.get_action_and_value(obs_t)\n",
        "                act_np = action.squeeze(0).cpu().numpy(); next_obs, shaped_r, term, trunc, info = self.env.step(act_np); done = term or trunc\n",
        "                self.buf_obs[t], self.buf_act[t], self.buf_logp[t], self.buf_rew[t], self.buf_done[t], self.buf_val[t] = obs, act_np, logp.item(), shaped_r, done, value.item()\n",
        "                obs, timesteps_collected = next_obs, timesteps_collected + 1\n",
        "                if \"episode_raw\" in info:\n",
        "                    ret = info[\"episode_raw\"][\"r\"]\n",
        "                    self.history['steps'].append(timesteps_collected)\n",
        "                    self.history['returns'].append(ret)\n",
        "                    self.history['phase'].append(1) # Phase 1 for Energy FT\n",
        "                    print(f\"FT Upd {update_count} | Step {timesteps_collected} | RAW {ret:7.2f}\")\n",
        "                if done: obs, _ = self.env.reset()\n",
        "            with torch.no_grad(): next_val = self.ac.get_value(torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)).item()\n",
        "            adv = np.zeros(self.rollout_steps, dtype=np.float32); lastgaelam = 0.0\n",
        "            for t in reversed(range(self.rollout_steps)):\n",
        "                nonterminal = 1.0 - self.buf_done[t]; next_v = next_val if t == self.rollout_steps-1 else self.buf_val[t+1]\n",
        "                delta = self.buf_rew[t] + self.gamma * next_v * nonterminal - self.buf_val[t]\n",
        "                lastgaelam = delta + self.gamma * self.lam * nonterminal * lastgaelam; adv[t] = lastgaelam\n",
        "            ret = adv + self.buf_val; b_obs, b_act, b_log, b_adv, b_ret = [torch.tensor(x, dtype=torch.float32, device=self.device) for x in [self.buf_obs, self.buf_act, self.buf_logp, adv, ret]]\n",
        "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
        "            for _ in range(self.train_epochs):\n",
        "                inds = np.arange(self.rollout_steps); np.random.shuffle(inds)\n",
        "                for start in range(0, self.rollout_steps, self.minibatch_size):\n",
        "                    mb = inds[start:start+self.minibatch_size]\n",
        "                    _, n_lp, ent, n_v = self.ac.get_action_and_value(b_obs[mb], b_act[mb]); ratio = torch.exp(n_lp - b_log[mb])\n",
        "                    surr1, surr2 = ratio * b_adv[mb], torch.clamp(ratio, 1.0-self.clip_eps, 1.0+self.clip_eps) * b_adv[mb]\n",
        "                    loss = -torch.min(surr1, surr2).mean() + 0.5 * ((n_v - b_ret[mb])**2).mean() - 0.0 * ent.mean()\n",
        "                    self.optimizer.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.ac.parameters(), self.max_grad_norm); self.optimizer.step()\n",
        "            self.lr_scheduler.step(); update_count += 1\n",
        "            if update_count % 50 == 0: self.save_checkpoint(CHECKPOINT_OUT, update_count, timesteps_collected)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = HardcoreEnergyFineTuner(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    agent.train()"
      ],
      "metadata": {
        "id": "0Y5JYKpI8Sel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Current version that can get 300, still working on it, uses multiple environments in parallel and oversamples runs that the agent does bad on, still working on it, if you run it let it run until it plateaus and then manually turn on the smoothness regularisers by setting the median threshold to a low value like 0, I have not figured out yet when a good time is to automatically turn it on"
      ],
      "metadata": {
        "id": "PJpF2mRf8Ycn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, io, base64, pickle, random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from collections import deque\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import gymnasium.wrappers.utils\n",
        "    torch.serialization.add_safe_globals([gymnasium.wrappers.utils.RunningMeanStd])\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "DRIVE_FOLDER = \"/content/drive/MyDrive/BipedalWalker_Project\"\n",
        "os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "BASE_NORMAL_MODEL = os.path.join(DRIVE_FOLDER, \"ppo_bipedal_final.pt\")\n",
        "CKPT_PATH  = os.path.join(DRIVE_FOLDER, \"ppo_hardcore_vector_hardseed_checkpoint.pt\")\n",
        "BEST_PATH  = os.path.join(DRIVE_FOLDER, \"ppo_hardcore_vector_hardseed_best.pt\")\n",
        "FINAL_PATH = os.path.join(DRIVE_FOLDER, \"ppo_hardcore_vector_hardseed_final.pt\")\n",
        "PLOT_PATH  = os.path.join(DRIVE_FOLDER, \"training_progress.png\")\n",
        "\n",
        "def show_video(folder=\"videos\"):\n",
        "    mp4list = glob.glob(f\"{folder}/*.mp4\")\n",
        "    if len(mp4list) == 0:\n",
        "        print(\"No video found.\")\n",
        "        return\n",
        "    mp4 = max(mp4list, key=os.path.getctime)\n",
        "    video = io.open(mp4, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    display(HTML(data=f\"\"\"\n",
        "    <video autoplay loop controls style=\"height: 420px;\">\n",
        "      <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    nn.init.orthogonal_(layer.weight, std)\n",
        "    nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 1), std=1.0),\n",
        "        )\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, 512)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(512, act_dim), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.ones(1, act_dim) * -0.7)\n",
        "\n",
        "    def get_value(self, x): return self.critic(x).squeeze(1)\n",
        "    def get_dist(self, x):\n",
        "        mean = self.actor_mean(x)\n",
        "        std = torch.exp(self.actor_logstd.expand_as(mean))\n",
        "        return Normal(mean, std), mean\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        dist, mean = self.get_dist(x)\n",
        "        if action is None: action = dist.sample()\n",
        "        return action, dist.log_prob(action).sum(1), dist.entropy().sum(1), self.get_value(x), mean\n",
        "\n",
        "def make_one_env(seed=None, render_mode=None):\n",
        "    env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode=render_mode)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env = gym.wrappers.ClipAction(env)\n",
        "    env = gym.wrappers.NormalizeObservation(env)\n",
        "    env = gym.wrappers.TransformObservation(env, lambda o: np.clip(o, -10, 10), env.observation_space)\n",
        "    if seed is not None: env.reset(seed=int(seed))\n",
        "    return env\n",
        "\n",
        "def make_vec_env(n, seed_list=None):\n",
        "    def thunk(i):\n",
        "        def _f():\n",
        "            s = None if seed_list is None else seed_list[i]\n",
        "            return make_one_env(seed=s)\n",
        "        return _f\n",
        "    return gym.vector.SyncVectorEnv([thunk(i) for i in range(n)])\n",
        "\n",
        "def find_normalize_obs_wrapper(env):\n",
        "    env_ptr = env\n",
        "    while hasattr(env_ptr, \"env\"):\n",
        "        if isinstance(env_ptr, gym.wrappers.NormalizeObservation): return env_ptr\n",
        "        env_ptr = env_ptr.env\n",
        "    return None\n",
        "\n",
        "class HardSeedPool:\n",
        "    def __init__(self, maxlen=4000):\n",
        "        self.maxlen = maxlen\n",
        "        self.seeds = deque(maxlen=maxlen)\n",
        "    def add(self, seed): self.seeds.append(int(seed))\n",
        "    def sample(self, k):\n",
        "        if not self.seeds: return None\n",
        "        return random.sample(list(self.seeds), min(k, len(self.seeds)))\n",
        "    def state_dict(self): return {\"maxlen\": self.maxlen, \"seeds\": list(self.seeds)}\n",
        "    def load_state_dict(self, d):\n",
        "        self.maxlen = int(d.get(\"maxlen\", self.maxlen))\n",
        "        self.seeds = deque(d.get(\"seeds\", []), maxlen=self.maxlen)\n",
        "\n",
        "class PPOHardcoreVector:\n",
        "    def __init__(self, device=\"cuda\", num_envs=16, rollout_steps=2048, total_env_steps=40_000_000, **kwargs):\n",
        "        self.device = device\n",
        "        self.num_envs = num_envs\n",
        "        self.rollout_steps = rollout_steps\n",
        "        self.total_env_steps = total_env_steps\n",
        "\n",
        "        self.gamma, self.lam, self.clip_eps = 0.99, 0.95, 0.2\n",
        "        self.lr, self.train_epochs, self.minibatch_size = 2.5e-4, 10, 4096\n",
        "        self.vf_coef, self.max_grad_norm, self.target_kl = 0.5, 0.5, 0.03\n",
        "        self.ent_coef_start, self.ent_coef_end = 0.01, 0.0\n",
        "\n",
        "        self.smooth_start_median = kwargs.get(\"smooth_start_median\", 120.0)\n",
        "        self.smooth_a_coef, self.smooth_da_coef = 0.002, 0.006\n",
        "        self.eval_every_updates, self.eval_episodes = 10, 16\n",
        "        self.hard_seed_threshold = 230.0\n",
        "\n",
        "        self.history = {\"steps\": [], \"returns\": [], \"smooth_on_step\": None}\n",
        "        self.seed_pool = HardSeedPool()\n",
        "        self.env_seeds = [random.randint(0, 2000000000) for _ in range(num_envs)]\n",
        "        self.env = make_vec_env(num_envs, seed_list=self.env_seeds)\n",
        "\n",
        "        self.ac = ActorCritic(self.env.single_observation_space.shape[0], self.env.single_action_space.shape[0]).to(device)\n",
        "        self.opt = optim.Adam(self.ac.parameters(), lr=self.lr, eps=1e-5)\n",
        "\n",
        "        self.global_step, self.update, self.best_eval_median, self.enable_smooth = 0, 0, -1e9, False\n",
        "\n",
        "        if os.path.exists(CKPT_PATH):\n",
        "            self.load(CKPT_PATH)\n",
        "        elif os.path.exists(BASE_NORMAL_MODEL):\n",
        "            self.ac.load_state_dict(torch.load(BASE_NORMAL_MODEL, map_location=device, weights_only=False))\n",
        "            with torch.no_grad(): self.ac.actor_logstd.fill_(-0.7)\n",
        "\n",
        "        self.buf_obs = np.zeros((rollout_steps, num_envs, self.env.single_observation_space.shape[0]), dtype=np.float32)\n",
        "        self.buf_act = np.zeros((rollout_steps, num_envs, self.env.single_action_space.shape[0]), dtype=np.float32)\n",
        "        self.buf_logp, self.buf_rew, self.buf_done, self.buf_val = np.zeros((4, rollout_steps, num_envs), dtype=np.float32)\n",
        "        self.prev_mean = np.zeros((num_envs, self.env.single_action_space.shape[0]), dtype=np.float32)\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        if len(self.history[\"returns\"]) < 2: return\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        steps, rets = self.history[\"steps\"], self.history[\"returns\"]\n",
        "        plt.plot(steps, rets, alpha=0.3, color='royalblue', label='Episode Return')\n",
        "        if len(rets) >= 50:\n",
        "            plt.plot(steps[49:], np.convolve(rets, np.ones(50)/50, mode='valid'), color='red', linewidth=2, label='Running Avg (50 eps)')\n",
        "        if self.history.get(\"smooth_on_step\"):\n",
        "            plt.axvline(x=self.history[\"smooth_on_step\"], color='green', linestyle='--', linewidth=2, label=\"Smoothness ON\")\n",
        "        plt.title(f\"BipedalWalker Progress (Step: {self.global_step})\"); plt.xlabel(\"Steps\"); plt.ylabel(\"Return\"); plt.grid(True, alpha=0.3); plt.legend()\n",
        "        plt.savefig(PLOT_PATH); plt.show()\n",
        "\n",
        "    def visualize_agent(self, update_count):\n",
        "        print(f\"\\n--- Visualizing Agent at Update {update_count} ---\")\n",
        "        vis_env = make_one_env(render_mode=\"rgb_array\")\n",
        "        vis_env = gym.wrappers.RecordVideo(vis_env, video_folder=\"videos\", name_prefix=f\"hc_upd_{update_count}\", disable_logger=True)\n",
        "\n",
        "        train_wno = find_normalize_obs_wrapper(self.env.envs[0])\n",
        "        vis_wno = find_normalize_obs_wrapper(vis_env)\n",
        "        if train_wno and vis_wno: vis_wno.obs_rms = train_wno.obs_rms\n",
        "\n",
        "        obs, _ = vis_env.reset()\n",
        "        ep_ret, done = 0.0, False\n",
        "        while not done:\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad(): _, m = self.ac.get_dist(obs_t)\n",
        "            obs, r, term, trunc, _ = vis_env.step(m.squeeze(0).cpu().numpy())\n",
        "            ep_ret += float(r); done = term or trunc\n",
        "        vis_env.close()\n",
        "        print(f\"Visualization finished with return: {ep_ret:.2f}\")\n",
        "        show_video(\"videos\")\n",
        "\n",
        "    def save(self, path):\n",
        "        wno = find_normalize_obs_wrapper(self.env.envs[0])\n",
        "        ckpt = {\n",
        "            \"model\": self.ac.state_dict(), \"opt\": self.opt.state_dict(), \"global_step\": self.global_step,\n",
        "            \"update\": self.update, \"best_eval_median\": self.best_eval_median, \"enable_smooth\": self.enable_smooth,\n",
        "            \"obs_rms\": (wno.obs_rms if wno else None), \"seed_pool\": self.seed_pool.state_dict(),\n",
        "            \"env_seeds\": self.env_seeds, \"history\": self.history\n",
        "        }\n",
        "        torch.save(ckpt, path)\n",
        "        self.plot_training_progress()\n",
        "\n",
        "    def load(self, path):\n",
        "        ckpt = torch.load(path, map_location=self.device, weights_only=False)\n",
        "        self.ac.load_state_dict(ckpt[\"model\"])\n",
        "        if \"opt\" in ckpt: self.opt.load_state_dict(ckpt[\"opt\"])\n",
        "        self.global_step, self.update = ckpt[\"global_step\"], ckpt[\"update\"]\n",
        "        self.best_eval_median = ckpt.get(\"best_eval_median\", -1e9)\n",
        "        self.enable_smooth = ckpt.get(\"enable_smooth\", False)\n",
        "        self.history = ckpt.get(\"history\", {\"steps\": [], \"returns\": [], \"smooth_on_step\": None})\n",
        "        if \"smooth_on_step\" not in self.history:\n",
        "            self.history[\"smooth_on_step\"] = self.global_step if self.enable_smooth else None\n",
        "        if ckpt.get(\"seed_pool\"): self.seed_pool.load_state_dict(ckpt[\"seed_pool\"])\n",
        "        if ckpt.get(\"env_seeds\"): self.env_seeds = list(ckpt[\"env_seeds\"])\n",
        "        try: self.env.close()\n",
        "        except: pass\n",
        "        self.env = make_vec_env(self.num_envs, seed_list=self.env_seeds)\n",
        "        if ckpt.get(\"obs_rms\"):\n",
        "            for e in self.env.envs:\n",
        "                wno = find_normalize_obs_wrapper(e)\n",
        "                if wno: wno.obs_rms = ckpt[\"obs_rms\"]\n",
        "        print(f\"Resumed: update={self.update} step={self.global_step}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, episodes=16):\n",
        "        env = make_one_env()\n",
        "        t_wno, e_wno = find_normalize_obs_wrapper(self.env.envs[0]), find_normalize_obs_wrapper(env)\n",
        "        if t_wno and e_wno: e_wno.obs_rms = t_wno.obs_rms\n",
        "        rets = []\n",
        "        for _ in range(episodes):\n",
        "            seed = random.randint(0, 2000000000)\n",
        "            obs, _ = env.reset(seed=int(seed))\n",
        "            ep_ret, done = 0.0, False\n",
        "            while not done:\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                _, m = self.ac.get_dist(obs_t)\n",
        "                obs, r, term, trunc, _ = env.step(m.squeeze(0).cpu().numpy())\n",
        "                ep_ret += float(r); done = term or trunc\n",
        "            rets.append(ep_ret)\n",
        "        env.close()\n",
        "        return np.mean(rets), np.median(rets), np.max(rets), np.min(rets)\n",
        "\n",
        "    def train(self):\n",
        "        obs, _ = self.env.reset(); self.prev_mean[:] = 0.0\n",
        "        batch_size = self.num_envs * self.rollout_steps\n",
        "        while self.global_step < self.total_env_steps:\n",
        "            with torch.no_grad(): self.ac.actor_logstd.clamp_(-5.0, -1.2)\n",
        "            ent_coef = self.ent_coef_end + (self.ent_coef_start - self.ent_coef_end) * max(0.0, 1.0 - (self.global_step / self.total_env_steps))\n",
        "\n",
        "            for t in range(self.rollout_steps):\n",
        "                self.buf_obs[t] = obs\n",
        "                with torch.no_grad(): action, logp, ent, val, mean = self.ac.get_action_and_value(torch.tensor(obs, dtype=torch.float32, device=self.device))\n",
        "                act_np, mean_np = action.cpu().numpy(), mean.cpu().numpy()\n",
        "                next_obs, raw_r, term, trunc, infos = self.env.step(act_np); done = np.logical_or(term, trunc)\n",
        "                shaped = raw_r.astype(np.float32)\n",
        "                if self.enable_smooth:\n",
        "                    shaped -= (self.smooth_a_coef * np.mean(mean_np**2, axis=1) + self.smooth_da_coef * np.mean((mean_np - self.prev_mean)**2, axis=1)).astype(np.float32)\n",
        "                self.prev_mean, self.buf_act[t], self.buf_logp[t], self.buf_val[t], self.buf_rew[t], self.buf_done[t] = mean_np, act_np, logp.cpu().numpy(), val.cpu().numpy(), shaped, done.astype(np.float32)\n",
        "                obs = next_obs; self.global_step += self.num_envs\n",
        "                if isinstance(infos, dict) and \"episode\" in infos:\n",
        "                    for i in range(self.num_envs):\n",
        "                        if done[i]:\n",
        "                            r_val = float(infos[\"episode\"][\"r\"][i])\n",
        "                            self.history[\"steps\"].append(self.global_step); self.history[\"returns\"].append(r_val)\n",
        "                            print(f\"[TRAIN EP] step={self.global_step:9d} | upd={self.update:6d} | env={i:2d} | ret={r_val:7.2f} | smooth={'ON' if self.enable_smooth else 'OFF'} | ent={ent_coef:.4f}\")\n",
        "                            if r_val < self.hard_seed_threshold: self.seed_pool.add(self.env_seeds[i])\n",
        "                done_idxs = np.where(done)[0]\n",
        "                if len(done_idxs) > 0:\n",
        "                    hard = self.seed_pool.sample(len(done_idxs))\n",
        "                    for j, env_i in enumerate(done_idxs):\n",
        "                        seed = hard[j % len(hard)] if (hard and random.random() < 0.3) else random.randint(0, 2000000000)\n",
        "                        self.env_seeds[env_i] = seed; self.env.envs[env_i].reset(seed=int(seed))\n",
        "\n",
        "            with torch.no_grad(): nv = self.ac.get_value(torch.tensor(obs, dtype=torch.float32, device=self.device)).cpu().numpy()\n",
        "            adv = np.zeros((self.rollout_steps, self.num_envs), dtype=np.float32); lastg = 0\n",
        "            for t in reversed(range(self.rollout_steps)):\n",
        "                nt = 1.0 - self.buf_done[t]; v_next = nv if t == self.rollout_steps-1 else self.buf_val[t+1]\n",
        "                delta = self.buf_rew[t] + 0.99 * v_next * nt - self.buf_val[t]\n",
        "                lastg = delta + 0.99 * 0.95 * nt * lastg; adv[t] = lastg\n",
        "            ret = adv + self.buf_val\n",
        "            b_obs, b_act, b_log, b_adv, b_ret = [torch.tensor(x.reshape(batch_size, -1) if len(x.shape)>2 else x.reshape(-1), device=self.device) for x in [self.buf_obs, self.buf_act, self.buf_logp, adv, ret]]\n",
        "            b_adv = (b_adv - b_adv.mean()) / (b_adv.std() + 1e-8)\n",
        "            for g in self.opt.param_groups: g[\"lr\"] = self.lr\n",
        "            for _ in range(self.train_epochs):\n",
        "                inds = np.arange(batch_size); np.random.shuffle(inds)\n",
        "                for s in range(0, batch_size, 4096):\n",
        "                    mb = inds[s:s+4096]\n",
        "                    _, n_lp, ent, n_v, _ = self.ac.get_action_and_value(b_obs[mb], b_act[mb])\n",
        "                    log_r = n_lp - b_log[mb]; ratio = torch.exp(log_r)\n",
        "                    surr1, surr2 = ratio * b_adv[mb], torch.clamp(ratio, 0.8, 1.2) * b_adv[mb]\n",
        "                    loss = -torch.min(surr1, surr2).mean() + 0.5*(n_v - b_ret[mb]).pow(2).mean() - ent_coef * ent.mean()\n",
        "                    self.opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.ac.parameters(), 0.5); self.opt.step()\n",
        "                    with torch.no_grad(): approx_kl = (ratio - 1.0 - log_r).mean().item()\n",
        "                    if approx_kl > 1.5 * 0.03: break\n",
        "                if approx_kl > 1.5 * 0.03: break\n",
        "            self.update += 1\n",
        "            if self.update % 10 == 0:\n",
        "                mean_r, med_r, best_r, worst_r = self.evaluate()\n",
        "                print(f\"Deterministic evaluation | mean {mean_r:7.2f} median {med_r:7.2f} best {best_r:7.2f} worst {worst_r:7.2f} | ent {ent_coef:.4f}\")\n",
        "                if (not self.enable_smooth) and (med_r >= self.smooth_start_median):\n",
        "                    self.enable_smooth, self.history[\"smooth_on_step\"] = True, self.global_step\n",
        "                if med_r > self.best_eval_median:\n",
        "                    self.best_eval_median = med_r; torch.save(self.ac.state_dict(), BEST_PATH)\n",
        "                self.save(CKPT_PATH)\n",
        "                self.visualize_agent(self.update)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    trainer = PPOHardcoreVector(device=device, smooth_start_median=120.0)\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "SeAfTXE98XlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}